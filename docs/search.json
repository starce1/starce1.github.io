[
  {
    "objectID": "stat_project.html",
    "href": "stat_project.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "The power of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false. In other words, it’s the probability that a statistical test can detect when a true difference exists. The power depends on a number of factors. We will explore how the factors of variability in the data and size of the true difference in means effects the power curves."
  },
  {
    "objectID": "stat_project.html#the-power-of-a-statistical-test",
    "href": "stat_project.html#the-power-of-a-statistical-test",
    "title": "Mini Project 2",
    "section": "",
    "text": "The power of a statistical test is the probability that it rejects the null hypothesis when the null hypothesis is false. In other words, it’s the probability that a statistical test can detect when a true difference exists. The power depends on a number of factors. We will explore how the factors of variability in the data and size of the true difference in means effects the power curves."
  },
  {
    "objectID": "stat_project.html#analysis",
    "href": "stat_project.html#analysis",
    "title": "Mini Project 2",
    "section": "Analysis",
    "text": "Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThese three graphs use simulated data where the mean in one distribution is 100 and mean in the other is determined by the mean difference where the mean is the difference higher than 100 (so the second mean in these three graphs are 103, 105, and 107). All of these plots have five curves with standard deviation ranging from 6 to 14 by intervals of 2. Thre are two lines for each standard deviation in each plot where the ones that are jagged are created from the simulated observations and the smooth ones are the “lines of best fit”.\nWhat we are looking for is how long it takes for these power curves to reach a power of 0.8 (ie. that the chance the simulated test can decect a significant difference is 80%). We are looking for this to see how big of a sample size we’d need in the real world for certain circumstances including the factors we are investigating.\nIt is evident from these plots that the higher the variability (standard deviation), the larger the required sample size is to prove a difference. This makes sense since it would probably be easier to prove a difference when things aren’t too different (variating) everytime you run the test.\nIt is also evident that a higher difference in means that a smaller sample size is needed to show a difference which is clear since the bigger the difference is the easier it should be to show that there is a difference."
  },
  {
    "objectID": "part1.html",
    "href": "part1.html",
    "title": "US States",
    "section": "",
    "text": "NOAA defines “violent tornadoes” as tornadoes rated EF4/F4 or higher. Lets see what the map of those looks like!\n\n\n\n\n\nWhat we gain from these maps is some information about tornadoes by state. The first map shows that they seem to happen just about everywhere, but since Texas is the biggest state it has by far the most. When putting into consideration whether tornadoes are violent or not we get a better idea of where bad tornadoes happen as well as the frequency of them."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nick Starcevich",
    "section": "",
    "text": "St. Olaf College | BA in Math with Statistics & Data Science concentration | 2026\nSpring Lake Park High School | 2022\n\n\n\nSt. Olaf College | Digital Scholarship Intern | 2023 - Present\nMathnasium | Math Tutor | 2021"
  },
  {
    "objectID": "index.html#nick-starcevich",
    "href": "index.html#nick-starcevich",
    "title": "Nick Starcevich",
    "section": "",
    "text": "St. Olaf College | BA in Math with Statistics & Data Science concentration | 2026\nSpring Lake Park High School | 2022\n\n\n\nSt. Olaf College | Digital Scholarship Intern | 2023 - Present\nMathnasium | Math Tutor | 2021"
  },
  {
    "objectID": "nyt_sentiment_analysis.html",
    "href": "nyt_sentiment_analysis.html",
    "title": "Mini Project Part 3",
    "section": "",
    "text": "In the past, I’ve heard people complain about how the news is always too negative and emotional to listen to or read and that it never has any happy stories. I have heard of this data set containing New York Times article titles in the 90s and 2000s. I wanted to see if this seem to be true with these titles, so I did some analysis on this data set to see whether this may be true or not.\n\nlibrary(tidyverse)\nlibrary(RTextTools) \nlibrary(tidytext)\ndata(NYTimes)\nNYtimes &lt;- NYTimes |&gt;\n  as_tibble() |&gt;\n  mutate(Date = dmy(Date))\n\n#explain why looking at upper case words in particular (upper case means bigger deal and use extra emotion)\nUppercaseNYT &lt;- NYtimes |&gt;\n  mutate(Title = as.character(Title)) |&gt;\n  filter(str_detect(Title, \"\\\\b[A-Z]+\\\\b\"))\n\n\nUppercaseNYT_clean &lt;- UppercaseNYT |&gt;\n  unnest_tokens(word, Title) \n\nIn the past, I’ve noticed that people tend to express more emotion using words containing all uppercase letters. In this analysis I compare sentiments in titles with lowercase letters to titles with uppercase letters, and I also compare those uppercase titles to subtitles within those titles to see if they are any different.\n\nNormal_title_NYT &lt;- NYtimes |&gt;\n  mutate(Title = as.character(Title)) |&gt;\n  anti_join(UppercaseNYT, by = c(\"Title\" = \"Title\"))\n\nNormal_title_NYT_clean &lt;- Normal_title_NYT |&gt;\n  unnest_tokens(word, Title) \n\nNYT_sentiments_values_lower &lt;- Normal_title_NYT_clean |&gt;\n  inner_join(get_sentiments(\"afinn\"))\n\n\n\nNYT_sentiments_values_upper &lt;- UppercaseNYT_clean |&gt;\n  inner_join(get_sentiments(\"afinn\"))\n\n\n\nPostColon &lt;- UppercaseNYT |&gt;\n  mutate(TitlesPostColon = str_extract(Title, \":.*\")) |&gt;\n  select(-Title) |&gt;\n  drop_na() |&gt;\n  mutate(TitlesPostColon = str_extract(TitlesPostColon, \"[A-Z].*\")) \n\nPostColon_clean &lt;- PostColon |&gt;\n  unnest_tokens(word, TitlesPostColon) \n\nNYT_sentiments_values_post_colon &lt;- PostColon_clean |&gt;\n  inner_join(get_sentiments(\"afinn\"))"
  },
  {
    "objectID": "nyt_sentiment_analysis.html#introduction",
    "href": "nyt_sentiment_analysis.html#introduction",
    "title": "Mini Project Part 3",
    "section": "",
    "text": "In the past, I’ve heard people complain about how the news is always too negative and emotional to listen to or read and that it never has any happy stories. I have heard of this data set containing New York Times article titles in the 90s and 2000s. I wanted to see if this seem to be true with these titles, so I did some analysis on this data set to see whether this may be true or not.\n\nlibrary(tidyverse)\nlibrary(RTextTools) \nlibrary(tidytext)\ndata(NYTimes)\nNYtimes &lt;- NYTimes |&gt;\n  as_tibble() |&gt;\n  mutate(Date = dmy(Date))\n\n#explain why looking at upper case words in particular (upper case means bigger deal and use extra emotion)\nUppercaseNYT &lt;- NYtimes |&gt;\n  mutate(Title = as.character(Title)) |&gt;\n  filter(str_detect(Title, \"\\\\b[A-Z]+\\\\b\"))\n\n\nUppercaseNYT_clean &lt;- UppercaseNYT |&gt;\n  unnest_tokens(word, Title) \n\nIn the past, I’ve noticed that people tend to express more emotion using words containing all uppercase letters. In this analysis I compare sentiments in titles with lowercase letters to titles with uppercase letters, and I also compare those uppercase titles to subtitles within those titles to see if they are any different.\n\nNormal_title_NYT &lt;- NYtimes |&gt;\n  mutate(Title = as.character(Title)) |&gt;\n  anti_join(UppercaseNYT, by = c(\"Title\" = \"Title\"))\n\nNormal_title_NYT_clean &lt;- Normal_title_NYT |&gt;\n  unnest_tokens(word, Title) \n\nNYT_sentiments_values_lower &lt;- Normal_title_NYT_clean |&gt;\n  inner_join(get_sentiments(\"afinn\"))\n\n\n\nNYT_sentiments_values_upper &lt;- UppercaseNYT_clean |&gt;\n  inner_join(get_sentiments(\"afinn\"))\n\n\n\nPostColon &lt;- UppercaseNYT |&gt;\n  mutate(TitlesPostColon = str_extract(Title, \":.*\")) |&gt;\n  select(-Title) |&gt;\n  drop_na() |&gt;\n  mutate(TitlesPostColon = str_extract(TitlesPostColon, \"[A-Z].*\")) \n\nPostColon_clean &lt;- PostColon |&gt;\n  unnest_tokens(word, TitlesPostColon) \n\nNYT_sentiments_values_post_colon &lt;- PostColon_clean |&gt;\n  inner_join(get_sentiments(\"afinn\"))"
  },
  {
    "objectID": "nyt_sentiment_analysis.html#analysis",
    "href": "nyt_sentiment_analysis.html#analysis",
    "title": "Mini Project Part 3",
    "section": "Analysis",
    "text": "Analysis\n\nNYT_sentiments_values_lower|&gt;\n  summarize(average_sentiment_value_lower = mean(value))\n\n# A tibble: 1 × 1\n  average_sentiment_value_lower\n                          &lt;dbl&gt;\n1                        -0.690\n\nNYT_sentiments_values_upper|&gt;\n  summarize(average_sentiment_value_caps = mean(value))\n\n# A tibble: 1 × 1\n  average_sentiment_value_caps\n                         &lt;dbl&gt;\n1                       -0.822\n\nNYT_sentiments_values_post_colon|&gt;\n  summarize(average_sentiment_value_subtitles = mean(value))\n\n# A tibble: 1 × 1\n  average_sentiment_value_subtitles\n                              &lt;dbl&gt;\n1                            -0.569\n\n\nThe sentimental value scale works as following: the higher the distance away from zero the values are, the more sentimental it is, and if the value is positive, the sentiments are positive sentiments while if the values are negative, they are negative sentiments. Now we can see that in all three cases that we have an average of slightly negative words being used in these titles whereas predicted the most sentimental titles are all upper ones. Even though the subtitles are also in all uppercase the sentiments are slightly more positive than the lowercase data.\n\nNYT_sentiments_values_lower |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(year) |&gt;\n  summarize(average_sentiment_value_lower = mean(value)) |&gt;\n  ggplot(aes(x = as.numeric(year), y = average_sentiment_value_lower)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Sentiment Value in NYT Lowercase Titles Each Year\", x = \"Year\", y = \"Average Sentiment Value\")\n\n\n\nNYT_sentiments_values_upper |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(year) |&gt;\n  summarize(average_sentiment_value_caps = mean(value)) |&gt;\n  ggplot(aes(x = as.numeric(year), y = average_sentiment_value_caps)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Sentiment Value in NYT Capitalized Titles Each Year\", x = \"Year\", y = \"Average Sentiment Value\")\n\n\n\nNYT_sentiments_values_post_colon |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(year) |&gt;\n  summarize(average_sentiment_value_subtitles = mean(value)) |&gt;\n  ggplot(aes(x = as.numeric(year), y = average_sentiment_value_subtitles)) +\n  geom_point() +\n  geom_line() +\n  labs(title = \"Sentiment Value in NYT Subtitles Each Year\", x = \"Year\", y = \"Average Sentiment Value\")\n\n\n\n\nHere we compare the sentimental value for each of our three categories by year. There aren’t any trends that are notable between the three that are similar. However, it is interesting to see if you look at the scales, that all uppercase titles are clearly more negatively sentimental than the others. It is also interesting to see that there are some average year values in the subtitles data that are slightly positive.\n\nNYT_sentiments_pos_neg_lower &lt;- Normal_title_NYT_clean |&gt;\n  inner_join(get_sentiments(\"bing\"))\n\nNYT_sentiments_pos_neg_upper &lt;- UppercaseNYT_clean |&gt;\n  inner_join(get_sentiments(\"bing\"))\n\nNYT_sentiments_pos_neg_post_colon &lt;- PostColon_clean |&gt;\n  inner_join(get_sentiments(\"bing\"))\n\nNow let’s move onto sentimental data where instead of assigning positive and negative sentimental values, we only look at positive and negative sentimental categories.\n\nNYT_sentiments_pos_neg_lower |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  ggplot(aes(x = sentiment)) +\n  geom_bar() +\n  labs(title = \"Number of Negative and Positive Sentimental Words in Lowercase Titles\", x = \"Sentiment Category\", y = \"Word Count\")\n\n\n\nNYT_sentiments_pos_neg_upper |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  ggplot(aes(x = sentiment)) +\n  geom_bar() +\n  labs(title = \"Number of Negative and Positive Sentimental Words in Capitalized Titles\", x = \"Sentiment Category\", y = \"Word Count\")\n\n\n\nNYT_sentiments_pos_neg_post_colon |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  ggplot(aes(x = sentiment)) +\n  geom_bar() +\n  labs(title = \"Number of Negative and Positive Sentimental Words in Subtitles\", x = \"Sentiment Category\", y = \"Word Count\")\n\n\n\n\nThe story that these plots tell showing the number of negative and positive sentimental words in each of the categories that we see is that speaking proportionally, there are far more negative, sentimental words being used in the titles containing uppercase words.\n\nNYT_sentiments_pos_neg_lower |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(sentiment, year) |&gt;\n  summarize(n =n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=year, y = n)) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(title = \"Number of Negative and Positive Sentimental Words in Lowercase Title by Years\", x = \"Year\", y = \"Word Count\")\n\n\n\nNYT_sentiments_pos_neg_upper |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(sentiment, year) |&gt;\n  summarize(n =n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=year, y = n)) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(title = \"Number of Negative and Positive Sentimental Words in Capitalized Title by Years\", x = \"Year\", y = \"Word Count\")\n\n\n\nNYT_sentiments_pos_neg_post_colon |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(sentiment, year) |&gt;\n  summarize(n =n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=year, y = n)) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(title = \"Number of Negative and Positive Sentimental Words in Subtitle by Years\", x = \"Year\", y = \"Word Count\")\n\n\n\n\nWhen looking at the same situation by year, it seems that the lowercase words were a preference for sentimental titles in 2002 while the uppercase words seem to be a preference for sentimental titles in 2003. The subtitles apparently tended to be more sentimental in the years 1996 and 2001. This is true for all the years stated for all these categories for both the positive and negative sentiments.\n\nNYT_sentiments_lower &lt;- Normal_title_NYT_clean |&gt;\n  inner_join(get_sentiments(\"nrc\"))\n\nNYT_sentiments_upper &lt;- UppercaseNYT_clean |&gt;\n  inner_join(get_sentiments(\"nrc\"))\n\nNYT_sentiments_post_colon &lt;- PostColon_clean |&gt;\n  inner_join(get_sentiments(\"nrc\"))\n\nNow, let’s move onto data containing sentiment categories.\n\nNYT_sentiments_lower |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  ggplot(aes(x = sentiment)) +\n  geom_bar() +\n  labs(title = \"Number of Sentimental Words in Lowercase Titles\", x = \"Sentiment Category\", y = \"Word Count\")\n\n\n\nNYT_sentiments_upper |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  ggplot(aes(x = sentiment)) +\n  geom_bar() +\n  labs(title = \"Number of Sentimental Words in Capitalized Titles\", x = \"Sentiment Category\", y = \"Word Count\")\n\n\n\nNYT_sentiments_post_colon |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  ggplot(aes(x = sentiment)) +\n  geom_bar() +\n  labs(title = \"Number of Sentimental Words in Subtitles\", x = \"Sentiment Category\", y = \"Word Count\")\n\n\n\n\nSome things to note with these three plots is that they all look similar. The categories of trust and fear are both also decently high for all three categories and probably contributes to the positive and negative categories the most. It is also worth noting that comparing the positive and negative categories here, there actually seems to be more positive words in the subtitles than negative, but this is not true for the other two categories.\n\nNYT_sentiments_lower |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(sentiment, year) |&gt;\n  summarize(n =n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=year, y = n)) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(title = \"Number of Sentimental Words in Lowercase Title by Years\", x = \"Year\", y = \"Word Count\")\n\n\n\nNYT_sentiments_upper |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(sentiment, year) |&gt;\n  summarize(n =n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=year, y = n)) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(title = \"Number of Sentimental Words in Capitalized Title by Years\", x = \"Year\", y = \"Word Count\")\n\n\n\nNYT_sentiments_post_colon |&gt;\n  mutate(year = str_sub(Date, 1, 4)) |&gt;\n  group_by(sentiment, year) |&gt;\n  summarize(n =n()) |&gt;\n  ungroup() |&gt;\n  ggplot(aes(x=year, y = n)) +\n  geom_col() +\n  facet_wrap(~sentiment) +\n  labs(title = \"Number of Sentimental Words in Subtitle by Years\", x = \"Year\", y = \"Word Count\")\n\n\n\n\nFor both the uppercase and lowercase data it seems like the sentiments categories of fear sadness anger and discussed are all very high on the same years. It is also worth noting that there is very little joy and surprise in all these years further showing the news does tend to be negative in terms of sentiments as predicted. It is tough to analyze this data for the subtitles since we have a small sample size and most of these sentiment categories seem to look similar. However, we still have the same trends of little joy and surprise as in the other two examples."
  },
  {
    "objectID": "nyt_sentiment_analysis.html#conclusion",
    "href": "nyt_sentiment_analysis.html#conclusion",
    "title": "Mini Project Part 3",
    "section": "Conclusion",
    "text": "Conclusion\nUppercase titles tend to be more negative than lowercase titles while even though the subtitles with uppercase words tend to be more neutral than the other two categories, they are still slightly negative."
  },
  {
    "objectID": "part2.html",
    "href": "part2.html",
    "title": "Wisconsin Districts",
    "section": "",
    "text": "This map represents the proportion of people who voted for republican candidates in 2016 in each Wisconsin district. The only three districts that ended up winning democrat are 2, 3, and 4 which won by a lot since they are darker blues. There is evidence of gerrymandering in this map since all republican winning districts were intentionally made to barely win republican (which is seen in this map by the extremely slight tints of red for republican winning districts) so the state could come out winning republican overall. The gerrymandering was especially bad where districts 3 and 4 were so lopsided for democrats that they were such large blowouts that no republican candidates even ran since many democrats were intentionally lumped into these districts by the republicans that drew them so they could gain an advantage."
  }
]